<!doctype html>
<html lang="en" class="no-js">
  <head>
    <meta charset="utf-8">
<link rel="icon" type="image/png" href="/blog/assets/images/favicon.png">
<!-- begin _includes/seo.html -->





<title>Building on Kubernetes: Ingress - Earthly Blog</title>
<meta name="description" content="Here at Earthly, we are building an internal platform on AWS using EKS. I talked to our lead architect Corey Larson about the decisions and trade offs he is making as he designs our platform.   What is an internal platform?   The internal platform is really the phrase I’ve always used in other places.  It’s the product that’s not really the product that you’re shipping.   A way to think of it is your own company’s internal cloud. What is the set of resources, tooling, workflows that you use to ship software to your company, which itself should really be a product that you ship to yourselves. So for Earthly, that means a whole stack of technology.   We’re using AWS, we’re using Kubernetes. We’re hiding some parts of Kubernetes that we don’t want to use from everyday usage, so we don’t go there. We’re making some trade-offs as to where we choose platform lock-in and where we’re not.  It’s things like that. And as we add more services the platform should develop to accommodate those things in the way we choose to work.   Isn’t Kubernetes supposed to be the platform? It can be, if you don’t want to customize it. But the problem space there is just so large. There’re so many API objects, and so many different ways to do things that you don’t want to necessarily use them all.  Otherwise your platform just gets really complicated. So, you need to choose the bits and pieces that you want to use there. And, tons of the plugins that you use will also have custom resource definitions, their own custom definitions of API objects, that’s on top of the ones that already exist. So, you likely want to use those instead of the native Kubernetes ones if your software you’re choosing uses those. And so it’s all of that, but then there’s even more.   Kubernetes doesn’t do monitoring. Kubernetes generates logs, but doesn’t do anything with them. Kubernetes doesn’t necessarily force a deployment process or an integration kind of a process onto you as well. So Kubernetes is really just an arrow in your toolkit that kind of lets you build the rest of that, in a really nice way.   The platform should cover from the moment source code is pushed to get all the way up to this code is running in production.   Kubernetes Ingest Strategies   What is Ingress and how do requests get to a service inside of Kubernetes? Ingress is the networking layer of how you get requests to your services from the outside. And Kubernetes doesn’t really ship with an Ingress controller by default that actually performs all those functions, they want you to bring your own. And there’s a ton of really interesting ones out there. There’s Nginx that does that, there’s people who use Caddy to do some of that stuff. There’s newer ones that are cloud native. They’re in that giant behemoth of a chart, the cloud native compute foundation puts up, that gets memed all the time on Twitter.   The way it works is a request comes from the internet and it’s going to hit some kind of a load balancer usually. We’re using one of Amazon’s application load balancers, but in the past I’ve just used even regular HAProxy or things like that. And from there, it bounces into your cluster. Your Ingress controller is going to pick it up, figure out where it should go, which services it should get routed to and route it to that service. And then, your service will generate a response and it follows these layers of the onion back out.   Which Ingress controller are we going to use?   The one’s like Traefik. It’s still like a traditional ingress controller. And I find that that’s just a little more straightforward, it’s less complex. It fits with the way I like to build services where you don’t have a giant sea of microservices, a little bit better.   What is a service mesh?   People took the opportunity with it being a new landscape with lots of containers in a large, orchestrated environment to build service meshes. Istio is out there, as one example there, and they have their own huge API with all their CRDs and everything else to create these service meshes that route things around and figure out who’s where. I personally find them pretty complex, and I like to avoid them. I like more standard comprehendible routing approaches.   I’ve usually seen people use Nginx. So why not Nginx, and why Traefik?   Back when I first was doing some Kubernetes work, we were spinning up our first cluster. We were in Azure and we were, they didn’t have a managed offering at the time we were using some deployment script from Azure team to get it up and running. And, I don’t remember what we were using for Ingress. I remember there was the Calico networking layer in there, but… We would do things, and then suddenly the cluster would just stop routing traffic entirely. And, among other things, we chose to move off of Azure. So we went to AWS, and we actually started trying to work with the Nginx proxy. And it uses your standard Nginx server configuration to do a lot of configuration. Personally, I always have to Google a lot on those configuration files, cause they can get pretty complex   But we tried Traefik, and it just kind of worked, and all of the options of things we wanted to do were first class citizens, in terms of route matching and everything else to get the Traefik where it needed to go. And so, we just stuck with it. At the time, the guy I was working with, he would make fun of me a lot, cause I liked to use Go. And I hesitated to suggest Traefik to them early on, because their mascot’s the go gopher with the traffic cones and everything else. And I didn’t want them to think I was more of a fan boy than I actually was. So I held off suggesting it to him for a while. So we spun our tires a little longer on Nginx than we should have. And I think it’s gotten better since then, but I just have enough knowledge around how Traefik works in it.   Have you ever used Nginx Lua scripting?   Infrastructure should be declarative, at least in my book. I don’t want a program that I have to then debug to figure out what my configuration actually is. For better or worse, a pile of YAML is not going to change on you once you ship it somewhere else, and it’s not going to have bugs. It’s all there, it’s all declarative. You can put it in Git, and see who did what, when, to your infrastructure.   When do you think we would consider using a service mesh?   I’ve seen it used in a lot of places where people just have a ton of different microservices. I’m not a fan of services that small. I’m a much bigger fan of slightly larger services that are, right-sized, to do things. And I’ve never found something like a service mesh to be useful, that useful in that kind of an environment.   What are other important decisions about ingress you had to consider?   We are terminating SSL in the ingress layer. That way your applications don’t necessarily have to worry about that. Pluses and minuses again, everything’s a trade-off. But that’s what we’re choosing to do just because it makes things a little more easily visible and debug-able.   The downside is if someone was in our cluster they could see what’s going on between services. But, that is the tradeoff we are making for now.   What about Deployment strategies?   Kubernetes has a few of those baked in, where it’ll roll through and you can set how many or maximum amount available, and how many extra can I spin up to give myself headroom while I’m rolling in a deployment. But there’s more to it than that because that does let you get to, more or less zero downtime.   Kubernetes will handle that, “Hey, no more traffic comes to this node.” And then we’ll spin it down as soon as all the traffic there is exhausted and spin up new ones, and let those get routed to. It’ll handle all that for you, and that’s really nice.   But, there’s more to it than that because there’s the whole adventure that takes place before it’s live in production taking traffic.   Testing in Production   What I’m hoping to do for us as we’re going forward here is, that we can stick new deployments out on production in some small way, and then use our Ingress controller to send it some traffic. Then we can actually test the new version of the code in production without affecting anybody as a canary before we choose to roll that actually out to everybody.   Service meshes are a little bit better at this, then simple ingress controllers. This is one place that service meshes do succeed, but Traefik can do it well enough too, where you actually split percentages of your request volume to one deployment versus the other.">


  <meta name="author" content="Corey Larson">
  
  <meta property="article:author" content="Corey Larson">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="en_US">
<meta property="og:site_name" content="Earthly Blog">
<meta property="og:title" content="Building on Kubernetes: Ingress">
<meta property="og:url" content="https://earthly.dev/blog/building-on-kubernetes-ingress/">


  <meta property="og:description" content="Here at Earthly, we are building an internal platform on AWS using EKS. I talked to our lead architect Corey Larson about the decisions and trade offs he is making as he designs our platform.   What is an internal platform?   The internal platform is really the phrase I’ve always used in other places.  It’s the product that’s not really the product that you’re shipping.   A way to think of it is your own company’s internal cloud. What is the set of resources, tooling, workflows that you use to ship software to your company, which itself should really be a product that you ship to yourselves. So for Earthly, that means a whole stack of technology.   We’re using AWS, we’re using Kubernetes. We’re hiding some parts of Kubernetes that we don’t want to use from everyday usage, so we don’t go there. We’re making some trade-offs as to where we choose platform lock-in and where we’re not.  It’s things like that. And as we add more services the platform should develop to accommodate those things in the way we choose to work.   Isn’t Kubernetes supposed to be the platform? It can be, if you don’t want to customize it. But the problem space there is just so large. There’re so many API objects, and so many different ways to do things that you don’t want to necessarily use them all.  Otherwise your platform just gets really complicated. So, you need to choose the bits and pieces that you want to use there. And, tons of the plugins that you use will also have custom resource definitions, their own custom definitions of API objects, that’s on top of the ones that already exist. So, you likely want to use those instead of the native Kubernetes ones if your software you’re choosing uses those. And so it’s all of that, but then there’s even more.   Kubernetes doesn’t do monitoring. Kubernetes generates logs, but doesn’t do anything with them. Kubernetes doesn’t necessarily force a deployment process or an integration kind of a process onto you as well. So Kubernetes is really just an arrow in your toolkit that kind of lets you build the rest of that, in a really nice way.   The platform should cover from the moment source code is pushed to get all the way up to this code is running in production.   Kubernetes Ingest Strategies   What is Ingress and how do requests get to a service inside of Kubernetes? Ingress is the networking layer of how you get requests to your services from the outside. And Kubernetes doesn’t really ship with an Ingress controller by default that actually performs all those functions, they want you to bring your own. And there’s a ton of really interesting ones out there. There’s Nginx that does that, there’s people who use Caddy to do some of that stuff. There’s newer ones that are cloud native. They’re in that giant behemoth of a chart, the cloud native compute foundation puts up, that gets memed all the time on Twitter.   The way it works is a request comes from the internet and it’s going to hit some kind of a load balancer usually. We’re using one of Amazon’s application load balancers, but in the past I’ve just used even regular HAProxy or things like that. And from there, it bounces into your cluster. Your Ingress controller is going to pick it up, figure out where it should go, which services it should get routed to and route it to that service. And then, your service will generate a response and it follows these layers of the onion back out.   Which Ingress controller are we going to use?   The one’s like Traefik. It’s still like a traditional ingress controller. And I find that that’s just a little more straightforward, it’s less complex. It fits with the way I like to build services where you don’t have a giant sea of microservices, a little bit better.   What is a service mesh?   People took the opportunity with it being a new landscape with lots of containers in a large, orchestrated environment to build service meshes. Istio is out there, as one example there, and they have their own huge API with all their CRDs and everything else to create these service meshes that route things around and figure out who’s where. I personally find them pretty complex, and I like to avoid them. I like more standard comprehendible routing approaches.   I’ve usually seen people use Nginx. So why not Nginx, and why Traefik?   Back when I first was doing some Kubernetes work, we were spinning up our first cluster. We were in Azure and we were, they didn’t have a managed offering at the time we were using some deployment script from Azure team to get it up and running. And, I don’t remember what we were using for Ingress. I remember there was the Calico networking layer in there, but… We would do things, and then suddenly the cluster would just stop routing traffic entirely. And, among other things, we chose to move off of Azure. So we went to AWS, and we actually started trying to work with the Nginx proxy. And it uses your standard Nginx server configuration to do a lot of configuration. Personally, I always have to Google a lot on those configuration files, cause they can get pretty complex   But we tried Traefik, and it just kind of worked, and all of the options of things we wanted to do were first class citizens, in terms of route matching and everything else to get the Traefik where it needed to go. And so, we just stuck with it. At the time, the guy I was working with, he would make fun of me a lot, cause I liked to use Go. And I hesitated to suggest Traefik to them early on, because their mascot’s the go gopher with the traffic cones and everything else. And I didn’t want them to think I was more of a fan boy than I actually was. So I held off suggesting it to him for a while. So we spun our tires a little longer on Nginx than we should have. And I think it’s gotten better since then, but I just have enough knowledge around how Traefik works in it.   Have you ever used Nginx Lua scripting?   Infrastructure should be declarative, at least in my book. I don’t want a program that I have to then debug to figure out what my configuration actually is. For better or worse, a pile of YAML is not going to change on you once you ship it somewhere else, and it’s not going to have bugs. It’s all there, it’s all declarative. You can put it in Git, and see who did what, when, to your infrastructure.   When do you think we would consider using a service mesh?   I’ve seen it used in a lot of places where people just have a ton of different microservices. I’m not a fan of services that small. I’m a much bigger fan of slightly larger services that are, right-sized, to do things. And I’ve never found something like a service mesh to be useful, that useful in that kind of an environment.   What are other important decisions about ingress you had to consider?   We are terminating SSL in the ingress layer. That way your applications don’t necessarily have to worry about that. Pluses and minuses again, everything’s a trade-off. But that’s what we’re choosing to do just because it makes things a little more easily visible and debug-able.   The downside is if someone was in our cluster they could see what’s going on between services. But, that is the tradeoff we are making for now.   What about Deployment strategies?   Kubernetes has a few of those baked in, where it’ll roll through and you can set how many or maximum amount available, and how many extra can I spin up to give myself headroom while I’m rolling in a deployment. But there’s more to it than that because that does let you get to, more or less zero downtime.   Kubernetes will handle that, “Hey, no more traffic comes to this node.” And then we’ll spin it down as soon as all the traffic there is exhausted and spin up new ones, and let those get routed to. It’ll handle all that for you, and that’s really nice.   But, there’s more to it than that because there’s the whole adventure that takes place before it’s live in production taking traffic.   Testing in Production   What I’m hoping to do for us as we’re going forward here is, that we can stick new deployments out on production in some small way, and then use our Ingress controller to send it some traffic. Then we can actually test the new version of the code in production without affecting anybody as a canary before we choose to roll that actually out to everybody.   Service meshes are a little bit better at this, then simple ingress controllers. This is one place that service meshes do succeed, but Traefik can do it well enough too, where you actually split percentages of your request volume to one deployment versus the other.">



  <meta property="og:image" content="/blog/generated/assets/images/building-on-kubernetes-ingress/header-800-0fe6e4762.jpg">



  <meta name="twitter:site" content="@EarthlyTech">
  <meta name="twitter:title" content="Building on Kubernetes: Ingress">
  <meta name="twitter:description" content="Here at Earthly, we are building an internal platform on AWS using EKS. I talked to our lead architect Corey Larson about the decisions and trade offs he is making as he designs our platform.   What is an internal platform?   The internal platform is really the phrase I’ve always used in other places.  It’s the product that’s not really the product that you’re shipping.   A way to think of it is your own company’s internal cloud. What is the set of resources, tooling, workflows that you use to ship software to your company, which itself should really be a product that you ship to yourselves. So for Earthly, that means a whole stack of technology.   We’re using AWS, we’re using Kubernetes. We’re hiding some parts of Kubernetes that we don’t want to use from everyday usage, so we don’t go there. We’re making some trade-offs as to where we choose platform lock-in and where we’re not.  It’s things like that. And as we add more services the platform should develop to accommodate those things in the way we choose to work.   Isn’t Kubernetes supposed to be the platform? It can be, if you don’t want to customize it. But the problem space there is just so large. There’re so many API objects, and so many different ways to do things that you don’t want to necessarily use them all.  Otherwise your platform just gets really complicated. So, you need to choose the bits and pieces that you want to use there. And, tons of the plugins that you use will also have custom resource definitions, their own custom definitions of API objects, that’s on top of the ones that already exist. So, you likely want to use those instead of the native Kubernetes ones if your software you’re choosing uses those. And so it’s all of that, but then there’s even more.   Kubernetes doesn’t do monitoring. Kubernetes generates logs, but doesn’t do anything with them. Kubernetes doesn’t necessarily force a deployment process or an integration kind of a process onto you as well. So Kubernetes is really just an arrow in your toolkit that kind of lets you build the rest of that, in a really nice way.   The platform should cover from the moment source code is pushed to get all the way up to this code is running in production.   Kubernetes Ingest Strategies   What is Ingress and how do requests get to a service inside of Kubernetes? Ingress is the networking layer of how you get requests to your services from the outside. And Kubernetes doesn’t really ship with an Ingress controller by default that actually performs all those functions, they want you to bring your own. And there’s a ton of really interesting ones out there. There’s Nginx that does that, there’s people who use Caddy to do some of that stuff. There’s newer ones that are cloud native. They’re in that giant behemoth of a chart, the cloud native compute foundation puts up, that gets memed all the time on Twitter.   The way it works is a request comes from the internet and it’s going to hit some kind of a load balancer usually. We’re using one of Amazon’s application load balancers, but in the past I’ve just used even regular HAProxy or things like that. And from there, it bounces into your cluster. Your Ingress controller is going to pick it up, figure out where it should go, which services it should get routed to and route it to that service. And then, your service will generate a response and it follows these layers of the onion back out.   Which Ingress controller are we going to use?   The one’s like Traefik. It’s still like a traditional ingress controller. And I find that that’s just a little more straightforward, it’s less complex. It fits with the way I like to build services where you don’t have a giant sea of microservices, a little bit better.   What is a service mesh?   People took the opportunity with it being a new landscape with lots of containers in a large, orchestrated environment to build service meshes. Istio is out there, as one example there, and they have their own huge API with all their CRDs and everything else to create these service meshes that route things around and figure out who’s where. I personally find them pretty complex, and I like to avoid them. I like more standard comprehendible routing approaches.   I’ve usually seen people use Nginx. So why not Nginx, and why Traefik?   Back when I first was doing some Kubernetes work, we were spinning up our first cluster. We were in Azure and we were, they didn’t have a managed offering at the time we were using some deployment script from Azure team to get it up and running. And, I don’t remember what we were using for Ingress. I remember there was the Calico networking layer in there, but… We would do things, and then suddenly the cluster would just stop routing traffic entirely. And, among other things, we chose to move off of Azure. So we went to AWS, and we actually started trying to work with the Nginx proxy. And it uses your standard Nginx server configuration to do a lot of configuration. Personally, I always have to Google a lot on those configuration files, cause they can get pretty complex   But we tried Traefik, and it just kind of worked, and all of the options of things we wanted to do were first class citizens, in terms of route matching and everything else to get the Traefik where it needed to go. And so, we just stuck with it. At the time, the guy I was working with, he would make fun of me a lot, cause I liked to use Go. And I hesitated to suggest Traefik to them early on, because their mascot’s the go gopher with the traffic cones and everything else. And I didn’t want them to think I was more of a fan boy than I actually was. So I held off suggesting it to him for a while. So we spun our tires a little longer on Nginx than we should have. And I think it’s gotten better since then, but I just have enough knowledge around how Traefik works in it.   Have you ever used Nginx Lua scripting?   Infrastructure should be declarative, at least in my book. I don’t want a program that I have to then debug to figure out what my configuration actually is. For better or worse, a pile of YAML is not going to change on you once you ship it somewhere else, and it’s not going to have bugs. It’s all there, it’s all declarative. You can put it in Git, and see who did what, when, to your infrastructure.   When do you think we would consider using a service mesh?   I’ve seen it used in a lot of places where people just have a ton of different microservices. I’m not a fan of services that small. I’m a much bigger fan of slightly larger services that are, right-sized, to do things. And I’ve never found something like a service mesh to be useful, that useful in that kind of an environment.   What are other important decisions about ingress you had to consider?   We are terminating SSL in the ingress layer. That way your applications don’t necessarily have to worry about that. Pluses and minuses again, everything’s a trade-off. But that’s what we’re choosing to do just because it makes things a little more easily visible and debug-able.   The downside is if someone was in our cluster they could see what’s going on between services. But, that is the tradeoff we are making for now.   What about Deployment strategies?   Kubernetes has a few of those baked in, where it’ll roll through and you can set how many or maximum amount available, and how many extra can I spin up to give myself headroom while I’m rolling in a deployment. But there’s more to it than that because that does let you get to, more or less zero downtime.   Kubernetes will handle that, “Hey, no more traffic comes to this node.” And then we’ll spin it down as soon as all the traffic there is exhausted and spin up new ones, and let those get routed to. It’ll handle all that for you, and that’s really nice.   But, there’s more to it than that because there’s the whole adventure that takes place before it’s live in production taking traffic.   Testing in Production   What I’m hoping to do for us as we’re going forward here is, that we can stick new deployments out on production in some small way, and then use our Ingress controller to send it some traffic. Then we can actually test the new version of the code in production without affecting anybody as a canary before we choose to roll that actually out to everybody.   Service meshes are a little bit better at this, then simple ingress controllers. This is one place that service meshes do succeed, but Traefik can do it well enough too, where you actually split percentages of your request volume to one deployment versus the other.">
  <meta name="twitter:url" content="https://earthly.dev/blog/building-on-kubernetes-ingress/">

  
    <meta name="twitter:card" content="summary_large_image">
    <meta name="twitter:image" content="https://earthly.dev/blog/generated/assets/images/building-on-kubernetes-ingress/header-800-0fe6e4762.jpg">
  

  



  <meta property="article:published_time" content="2020-10-22T00:00:00-04:00">





  

  


<link rel="canonical" href="https://earthly.dev/blog/building-on-kubernetes-ingress/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Organization",
      "url": "https://earthly.dev/blog/",
      "logo": "/assets/images/logo-header.png"
    
  }
</script>






<!-- end _includes/seo.html -->



  <link href="/blog/feed.xml" type="application/atom+xml" rel="alternate" title="Earthly Blog Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/blog/assets/css/main.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css">

<!--[if IE]>
  <style>
    /* old IE unsupported flexbox fixes */
    .greedy-nav .site-title {
      padding-right: 3em;
    }
    .greedy-nav button {
      position: absolute;
      top: 0;
      right: 0;
      height: 100%;
    }
  </style>
<![endif]-->


  
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-161831101-5"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-161831101-5');
</script>
  <!-- Facebook Pixel Code -->
<script>
    !function(f,b,e,v,n,t,s)
    {if(f.fbq)return;n=f.fbq=function(){n.callMethod?
    n.callMethod.apply(n,arguments):n.queue.push(arguments)};
    if(!f._fbq)f._fbq=n;n.push=n;n.loaded=!0;n.version='2.0';
    n.queue=[];t=b.createElement(e);t.async=!0;
    t.src=v;s=b.getElementsByTagName(e)[0];
    s.parentNode.insertBefore(t,s)}(window, document,'script',
    'https://connect.facebook.net/en_US/fbevents.js');
    fbq('init', '259843109045285');
    fbq('track', 'PageView');
    </script>
    <noscript><img height="1" width="1" style="display:none"
    src="https://www.facebook.com/tr?id=259843109045285&ev=PageView&noscript=1"
    />
</noscript>
 <!-- End Facebook Pixel Code -->
  <!-- Twitter universal website tag code -->
<script>
  !function(e,t,n,s,u,a){e.twq||(s=e.twq=function(){s.exe?s.exe.apply(s,arguments):s.queue.push(arguments);
  },s.version='1.1',s.queue=[],u=t.createElement(n),u.async=!0,u.src='//static.ads-twitter.com/uwt.js',
  a=t.getElementsByTagName(n)[0],a.parentNode.insertBefore(u,a))}(window,document,'script');
  // Insert Twitter Pixel ID and Standard Event data below
  twq('init','o5s6p');
  twq('track','PageView');
  </script>
  <!-- End Twitter universal website tag code -->


  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <h2 class="screen-reader-text">Skip links</h2>
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
          <a class="site-logo" href="/"><img src="/blog/assets/images/white-logo.png" alt="Earthly"></a>
        
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/blog/">Blog home</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/categories/articles/">Articles</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/categories/news/">News</a>
            </li><li class="masthead__menu-item">
              <a href="/blog/categories/tutorials/">Tutorials</a>
            </li><li class="masthead__menu-item">
              <a href="https://github.com/earthly/earthly">GitHub</a>
            </li></ul>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">Toggle menu</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      




  







<div class="page__hero"
  style=" background-image: url('');"
>
  
      <picture class="page__hero-image"><source srcset="/blog/generated/assets/images/building-on-kubernetes-ingress/header-400-4b7679a48.webp 400w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-600-4b7679a48.webp 600w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-800-4b7679a48.webp 800w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-1000-4b7679a48.webp 1000w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-1200-4b7679a48.webp 1200w" type="image/webp"><source srcset="/blog/generated/assets/images/building-on-kubernetes-ingress/header-400-4b7679a48.png 400w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-600-4b7679a48.png 600w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-800-4b7679a48.png 800w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-1000-4b7679a48.png 1000w, /blog/generated/assets/images/building-on-kubernetes-ingress/header-1200-4b7679a48.png 1200w" type="image/png"><img src="/blog/generated/assets/images/building-on-kubernetes-ingress/header-800-4b7679a48.jpg" alt="Building on Kubernetes: Ingress"></picture>

  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar">
  
  </div>



  <article class="page" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="Building on Kubernetes: Ingress">
    <meta itemprop="description" content="Here at Earthly, we are building an internal platform on AWS using EKS. I talked to our lead architect Corey Larson about the decisions and trade offs he is making as he designs our platform.What is an internal platform?The internal platform is really the phrase I’ve always used in other places.  It’s the product that’s not really the product that you’re shipping.A way to think of it is your own company’s internal cloud. What is the set of resources, tooling, workflows that you use to ship software to your company, which itself should really be a product that you ship to yourselves. So for Earthly, that means a whole stack of technology.We’re using AWS, we’re using Kubernetes. We’re hiding some parts of Kubernetes that we don’t want to use from everyday usage, so we don’t go there. We’re making some trade-offs as to where we choose platform lock-in and where we’re not.  It’s things like that. And as we add more services the platform should develop to accommodate those things in the way we choose to work.Isn’t Kubernetes supposed to be the platform? It can be, if you don’t want to customize it. But the problem space there is just so large. There’re so many API objects, and so many different ways to do things that you don’t want to necessarily use them all.  Otherwise your platform just gets really complicated. So, you need to choose the bits and pieces that you want to use there. And, tons of the plugins that you use will also have custom resource definitions, their own custom definitions of API objects, that’s on top of the ones that already exist. So, you likely want to use those instead of the native Kubernetes ones if your software you’re choosing uses those. And so it’s all of that, but then there’s even more.Kubernetes doesn’t do monitoring. Kubernetes generates logs, but doesn’t do anything with them. Kubernetes doesn’t necessarily force a deployment process or an integration kind of a process onto you as well. So Kubernetes is really just an arrow in your toolkit that kind of lets you build the rest of that, in a really nice way.The platform should cover from the moment source code is pushed to get all the way up to this code is running in production.Kubernetes Ingest StrategiesWhat is Ingress and how do requests get to a service inside of Kubernetes? Ingress is the networking layer of how you get requests to your services from the outside. And Kubernetes doesn’t really ship with an Ingress controller by default that actually performs all those functions, they want you to bring your own. And there’s a ton of really interesting ones out there. There’s Nginx that does that, there’s people who use Caddy to do some of that stuff. There’s newer ones that are cloud native. They’re in that giant behemoth of a chart, the cloud native compute foundation puts up, that gets memed all the time on Twitter.The way it works is a request comes from the internet and it’s going to hit some kind of a load balancer usually. We’re using one of Amazon’s application load balancers, but in the past I’ve just used even regular HAProxy or things like that. And from there, it bounces into your cluster. Your Ingress controller is going to pick it up, figure out where it should go, which services it should get routed to and route it to that service. And then, your service will generate a response and it follows these layers of the onion back out.Which Ingress controller are we going to use?The one’s like Traefik. It’s still like a traditional ingress controller. And I find that that’s just a little more straightforward, it’s less complex. It fits with the way I like to build services where you don’t have a giant sea of microservices, a little bit better.What is a service mesh?People took the opportunity with it being a new landscape with lots of containers in a large, orchestrated environment to build service meshes. Istio is out there, as one example there, and they have their own huge API with all their CRDs and everything else to create these service meshes that route things around and figure out who’s where. I personally find them pretty complex, and I like to avoid them. I like more standard comprehendible routing approaches.I’ve usually seen people use Nginx. So why not Nginx, and why Traefik?Back when I first was doing some Kubernetes work, we were spinning up our first cluster. We were in Azure and we were, they didn’t have a managed offering at the time we were using some deployment script from Azure team to get it up and running. And, I don’t remember what we were using for Ingress. I remember there was the Calico networking layer in there, but… We would do things, and then suddenly the cluster would just stop routing traffic entirely. And, among other things, we chose to move off of Azure. So we went to AWS, and we actually started trying to work with the Nginx proxy. And it uses your standard Nginx server configuration to do a lot of configuration. Personally, I always have to Google a lot on those configuration files, cause they can get pretty complexBut we tried Traefik, and it just kind of worked, and all of the options of things we wanted to do were first class citizens, in terms of route matching and everything else to get the Traefik where it needed to go. And so, we just stuck with it. At the time, the guy I was working with, he would make fun of me a lot, cause I liked to use Go. And I hesitated to suggest Traefik to them early on, because their mascot’s the go gopher with the traffic cones and everything else. And I didn’t want them to think I was more of a fan boy than I actually was. So I held off suggesting it to him for a while. So we spun our tires a little longer on Nginx than we should have. And I think it’s gotten better since then, but I just have enough knowledge around how Traefik works in it.Have you ever used Nginx Lua scripting?Infrastructure should be declarative, at least in my book. I don’t want a program that I have to then debug to figure out what my configuration actually is. For better or worse, a pile of YAML is not going to change on you once you ship it somewhere else, and it’s not going to have bugs. It’s all there, it’s all declarative. You can put it in Git, and see who did what, when, to your infrastructure.When do you think we would consider using a service mesh?I’ve seen it used in a lot of places where people just have a ton of different microservices. I’m not a fan of services that small. I’m a much bigger fan of slightly larger services that are, right-sized, to do things. And I’ve never found something like a service mesh to be useful, that useful in that kind of an environment.What are other important decisions about ingress you had to consider?We are terminating SSL in the ingress layer. That way your applications don’t necessarily have to worry about that. Pluses and minuses again, everything’s a trade-off. But that’s what we’re choosing to do just because it makes things a little more easily visible and debug-able.The downside is if someone was in our cluster they could see what’s going on between services. But, that is the tradeoff we are making for now.What about Deployment strategies?Kubernetes has a few of those baked in, where it’ll roll through and you can set how many or maximum amount available, and how many extra can I spin up to give myself headroom while I’m rolling in a deployment. But there’s more to it than that because that does let you get to, more or less zero downtime.Kubernetes will handle that, “Hey, no more traffic comes to this node.” And then we’ll spin it down as soon as all the traffic there is exhausted and spin up new ones, and let those get routed to. It’ll handle all that for you, and that’s really nice.But, there’s more to it than that because there’s the whole adventure that takes place before it’s live in production taking traffic.Testing in ProductionWhat I’m hoping to do for us as we’re going forward here is, that we can stick new deployments out on production in some small way, and then use our Ingress controller to send it some traffic. Then we can actually test the new version of the code in production without affecting anybody as a canary before we choose to roll that actually out to everybody.Service meshes are a little bit better at this, then simple ingress controllers. This is one place that service meshes do succeed, but Traefik can do it well enough too, where you actually split percentages of your request volume to one deployment versus the other.">
    <meta itemprop="datePublished" content="2020-10-22T00:00:00-04:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content" itemprop="text">
        
        <header>
          <h1 id="page-title" class="page__title" itemprop="headline">Building on Kubernetes: Ingress
</h1>
          

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          7 minute read
        
        
        &nbsp;	&nbsp;<i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2020-10-22T00:00:00-04:00">October 22, 2020</time>
        

      </span>
    
    <span>
      
      
      
      <div class="author__avatar_top">
          <picture class="image-author"><source srcset="/blog/generated/assets/images/authors/coreylarson-240-d9707b743.webp 240w" type="image/webp"><source srcset="/blog/generated/assets/images/authors/coreylarson-240-ebff39699.jpg 240w" type="image/jpeg"><img src="/blog/generated/assets/images/authors/coreylarson-240-ebff39699.jpg" alt="Corey Larson %"></picture>

          &nbsp;	&nbsp;
          Corey Larson
      </div>
      
    </span>
  </p>


        </header>
      
        
        <p><em>Here at Earthly, we are building an internal platform on AWS using EKS. I talked to our lead architect Corey Larson about the decisions and trade offs he is making as he designs our platform.</em></p>
<p><strong>What is an internal platform?</strong></p>
<p>The internal platform is really the phrase I’ve always used in other places.  It’s the product that’s not really the product that you’re shipping.</p>
<p>A way to think of it is your own company’s internal cloud. What is the set of resources, tooling, workflows that you use to ship software to your company, which itself should really be a product that you ship to yourselves. So for Earthly, that means a whole stack of technology.</p>
<p>We’re using AWS, we’re using Kubernetes. We’re hiding some parts of Kubernetes that we don’t want to use from everyday usage, so we don’t go there. We’re making some trade-offs as to where we choose platform lock-in and where we’re not.  It’s things like that. And as we add more services the platform should develop to accommodate those things in the way we choose to work.</p>
<p><strong>Isn’t Kubernetes supposed to be the platform?</strong><br />
It can be, if you don’t want to customize it. But the problem space there is just so large. There’re so many API objects, and so many different ways to do things that you don’t want to necessarily use them all.  Otherwise your platform just gets really complicated. So, you need to choose the bits and pieces that you want to use there. And, tons of the plugins that you use will also have custom resource definitions, their own custom definitions of API objects, that’s on top of the ones that already exist. So, you likely want to use those instead of the native Kubernetes ones if your software you’re choosing uses those. And so it’s all of that, but then there’s even more.</p>
<p>Kubernetes doesn’t do monitoring. Kubernetes generates logs, but doesn’t do anything with them. Kubernetes doesn’t necessarily force a deployment process or an integration kind of a process onto you as well. So Kubernetes is really just an arrow in your toolkit that kind of lets you build the rest of that, in a really nice way.</p>
<p>The platform should cover from the moment source code is pushed to get all the way up to this code is running in production.</p>
<h2 id="kubernetes-ingest-strategies">Kubernetes Ingest Strategies</h2>
<p><strong>What is Ingress and how do requests get to a service inside of Kubernetes?</strong><br />
Ingress is the networking layer of how you get requests to your services from the outside. And Kubernetes doesn’t really ship with an Ingress controller by default that actually performs all those functions, they want you to bring your own. And there’s a ton of really interesting ones out there. There’s Nginx that does that, there’s people who use Caddy to do some of that stuff. There’s newer ones that are cloud native. They’re in that giant behemoth of a chart, the cloud native compute foundation puts up, that gets memed all the time on Twitter.</p>
<p>The way it works is a request comes from the internet and it’s going to hit some kind of a load balancer usually. We’re using one of Amazon’s application load balancers, but in the past I’ve just used even regular HAProxy or things like that. And from there, it bounces into your cluster. Your Ingress controller is going to pick it up, figure out where it should go, which services it should get routed to and route it to that service. And then, your service will generate a response and it follows these layers of the onion back out.</p>
<p><strong>Which Ingress controller are we going to use?</strong></p>
<p>The one’s like Traefik. It’s still like a traditional ingress controller. And I find that that’s just a little more straightforward, it’s less complex. It fits with the way I like to build services where you don’t have a giant sea of microservices, a little bit better.</p>
<p><strong>What is a service mesh?</strong></p>
<p>People took the opportunity with it being a new landscape with lots of containers in a large, orchestrated environment to build service meshes. Istio is out there, as one example there, and they have their own huge API with all their CRDs and everything else to create these service meshes that route things around and figure out who’s where. I personally find them pretty complex, and I like to avoid them. I like more standard comprehendible routing approaches.</p>
<p><strong>I’ve usually seen people use Nginx. So why not Nginx, and why Traefik?</strong></p>
<p>Back when I first was doing some Kubernetes work, we were spinning up our first cluster. We were in Azure and we were, they didn’t have a managed offering at the time we were using some deployment script from Azure team to get it up and running. And, I don’t remember what we were using for Ingress. I remember there was the Calico networking layer in there, but… We would do things, and then suddenly the cluster would just stop routing traffic entirely. And, among other things, we chose to move off of Azure. So we went to AWS, and we actually started trying to work with the Nginx proxy. And it uses your standard Nginx server configuration to do a lot of configuration. Personally, I always have to Google a lot on those configuration files, cause they can get pretty complex</p>
<p>But we tried Traefik, and it just kind of worked, and all of the options of things we wanted to do were first class citizens, in terms of route matching and everything else to get the Traefik where it needed to go. And so, we just stuck with it. At the time, the guy I was working with, he would make fun of me a lot, cause I liked to use Go. And I hesitated to suggest Traefik to them early on, because their mascot’s the go gopher with the traffic cones and everything else. And I didn’t want them to think I was more of a fan boy than I actually was. So I held off suggesting it to him for a while. So we spun our tires a little longer on Nginx than we should have. And I think it’s gotten better since then, but I just have enough knowledge around how Traefik works in it.</p>
<p><strong>Have you ever used Nginx Lua scripting?</strong></p>
<p>Infrastructure should be declarative, at least in my book. I don’t want a program that I have to then debug to figure out what my configuration actually is. For better or worse, a pile of <a href="/blog/intercal-yaml-and-other-horrible-programming-languages">YAML</a> is not going to change on you once you ship it somewhere else, and it’s not going to have bugs. It’s all there, it’s all declarative. You can put it in Git, and see who did what, when, to your infrastructure.</p>
<p><strong>When do you think we would consider using a service mesh?</strong></p>
<p>I’ve seen it used in a lot of places where people just have a ton of different microservices. I’m not a fan of services that small. I’m a much bigger fan of slightly larger services that are, right-sized, to do things. And I’ve never found something like a service mesh to be useful, that useful in that kind of an environment.</p>
<p><strong>What are other important decisions about ingress you had to consider?</strong></p>
<p>We are terminating SSL in the ingress layer. That way your applications don’t necessarily have to worry about that. Pluses and minuses again, everything’s a trade-off. But that’s what we’re choosing to do just because it makes things a little more easily visible and debug-able.</p>
<p>The downside is if someone was in our cluster they could see what’s going on between services. But, that is the tradeoff we are making for now.</p>
<p><strong>What about Deployment strategies?</strong></p>
<p>Kubernetes has a few of those baked in, where it’ll roll through and you can set how many or maximum amount available, and how many extra can I spin up to give myself headroom while I’m rolling in a deployment. But there’s more to it than that because that does let you get to, more or less zero downtime.</p>
<p>Kubernetes will handle that, “Hey, no more traffic comes to this node.” And then we’ll spin it down as soon as all the traffic there is exhausted and spin up new ones, and let those get routed to. It’ll handle all that for you, and that’s really nice.</p>
<p>But, there’s more to it than that because there’s the whole adventure that takes place before it’s live in production taking traffic.</p>
<h2 id="testing-in-production">Testing in Production</h2>
<p>What I’m hoping to do for us as we’re going forward here is, that we can stick new deployments out on production in some small way, and then use our Ingress controller to send it some traffic. Then we can actually <a href="/blog/unit-vs-integration">test</a> the new version of the code in production without affecting anybody as a canary before we choose to roll that actually out to everybody.</p>
<p>Service meshes are a little bit better at this, then simple ingress controllers. This is one place that service meshes do succeed, but Traefik can do it well enough too, where you actually split percentages of your request volume to one deployment versus the other.</p>

        
      </section>

      


<div itemscope itemtype="https://schema.org/Person">
  <!-- <h5>About The Author</h5> -->

  
    <div class="author__avatar">
      
        <picture class="image-author"><source srcset="/blog/generated/assets/images/authors/coreylarson-240-d9707b743.webp 240w" type="image/webp"><source srcset="/blog/generated/assets/images/authors/coreylarson-240-ebff39699.jpg 240w" type="image/jpeg"><img src="/blog/generated/assets/images/authors/coreylarson-240-ebff39699.jpg" alt="Corey Larson %"></picture>

      
    </div>
  

  <div class="author__content">
    
      <h4 class="author__name" itemprop="name">
        Corey Larson
    
    
    </h4>
    
      <div class="author__bio" itemprop="description">
        <p>Eats, runs, and codes. Dad. Engineer. Progressive. LDS. Disneyland fanatic.</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <!-- <button class="btn btn--inverse">Follow</button> -->
    <ul class="author__urls social-icons">
      

     

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>

      <footer class="page__meta">
        
        

  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> Tags: </strong>
    <span itemprop="keywords">
    
      <a href="/blog/tags/#interview" class="page__taxonomy-item" rel="tag">Interview</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> Categories: </strong>
    <span itemprop="keywords">
    
      <a href="/blog/categories/articles" class="page__taxonomy-item" rel="tag">Articles</a>
    
    </span>
  </p>



        
  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> Updated:</strong> <time datetime="2020-10-22T00:00:00-04:00">October 22, 2020</time></p>


      </footer>

      

      
  <nav class="pagination">
    
      <a href="/blog/grateful-for-hacktoberfest/" class="pagination--pager" title="Grateful for Hacktoberfest!
">Previous</a>
    
    
      <a href="/blog/dont-be-weird/" class="pagination--pager" title="Technology choice? Don’t be weird
">Next</a>
    
  </nav>

    </div>
  </article>

  
  
    <div class="page__related">
      <h4 class="page__related-title">You may also enjoy</h4>
      <div class="grid__wrapper">
        
          










<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <picture><source srcset="/blog/generated/assets/images/example/teaser-600-fc9eab451.webp 600w, /blog/generated/assets/images/example/teaser-800-fc9eab451.webp 800w" type="image/webp"><source srcset="/blog/generated/assets/images/example/teaser-600-b449882ef.jpg 600w, /blog/generated/assets/images/example/teaser-800-b449882ef.jpg 800w" type="image/jpeg"><img src="/blog/generated/assets/images/example/teaser-800-b449882ef.jpg"></picture>

      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/example/" rel="permalink">Example Post
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          4 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

This post is in the future, and won’t show up in the published site


Image without figure


An image with the alt text hidden.





An image with alt text...</p>
  </article>
</div>

        
          










<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
      <div class="archive__item-teaser">
        <picture><source srcset="/blog/generated/assets/images/golang-makefile/header-600-c2a6f06af.webp 600w, /blog/generated/assets/images/golang-makefile/header-800-c2a6f06af.webp 800w" type="image/webp"><source srcset="/blog/generated/assets/images/golang-makefile/header-600-a3886f2c1.jpg 600w, /blog/generated/assets/images/golang-makefile/header-800-a3886f2c1.jpg 800w" type="image/jpeg"><img src="/blog/generated/assets/images/golang-makefile/header-800-a3886f2c1.jpg"></picture>

      </div>
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/blog/golang-makefile/" rel="permalink">Creating a Golang Makefile
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          6 minute read
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">

Building and testing any large codebase is time-consuming, error-prone, and repetitive. Golang supports multi-platform builds, which is excellent, but it n...</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>

    </div>

    

    <div id="footer" class="page__footer">
      <footer>
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>Follow:</strong></li>
    

    
      
        
          <li><a href="https://twitter.com/earthlytech" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-twitter-square" aria-hidden="true"></i> Twitter</a></li>
        
      
        
          <li><a href="https://github.com/earthly/earthly" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
    

    
      <li><a href="/blog/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> Feed</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2021 <a href="https://earthly.dev/">Earthly</a>. </div>

      </footer>
    </div>

    <script src="/blog/assets/js/vendor/jquery/jquery-3.5.1.js"></script>
<script src="/blog/assets/js/plugins/jquery.fitvids.js"></script>
<script src="/blog/assets/js/plugins/smooth-scroll.js"></script>
<script src="/blog/assets/js/plugins/gumshoe.js"></script>
<script src="/blog/assets/js/plugins/jquery.magnific-popup.js"></script>
<script src="/blog/assets/js/main.min.js"></script>

  <script> // minified version of https://earthly.dev/assets/js/analytics.js
    function setCookie(e,t,r){var i="";if(r){var o=new Date;o.setTime(o.getTime()+24*r*60*60*1e3),i="; expires="+o.toUTCString()}document.cookie=e+"="+(t||"")+i+"; path=/"}function getCookie(e){for(var t=e+"=",r=document.cookie.split(";"),i=0;i<r.length;i++){for(var o=r[i];" "==o.charAt(0);)o=o.substring(1,o.length);if(0==o.indexOf(t))return o.substring(t.length,o.length)}return null}function uuidv4(){return([1e7]+-1e3+-4e3+-8e3+-1e11).replace(/[018]/g,e=>(e^crypto.getRandomValues(new Uint8Array(1))[0]&15>>e/4).toString(16))}function getAnalyticCookie(){cookieName="earthlyID";var e=getCookie(cookieName);return null==e&&(e=uuidv4()),setCookie(cookieName,e,36500),e}jQuery.ajax({type:"POST",url:"https://api.earthly.dev/analytics",data:JSON.stringify({key:"website",url:window.location.href,referrer:document.referrer,earthlyID:getAnalyticCookie()})});
</script>




  </body>
</html>
